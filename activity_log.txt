
#1

Tried following instructions at:

https://superset.apache.org/docs/installation/installing-superset-using-docker-compose/

...didn't work

Moved on to instructions at:

https://hub.docker.com/r/apache/superset

...more success, was able to login, see some sample data.

Stopped container. Realised need to connect to real SQL database to import custom data.


#2

Installed Postgres DB to host.

Began building Dockerfile with hopes of connecting to Postgres DB on host.

Used the dependency list at:

https://superset.apache.org/docs/databases/installing-database-drivers

Found guides for opening up Postgres to new connections:

https://superset.apache.org/docs/installation/installing-superset-using-docker-compose/

^ at bottom of page

https://gist.github.com/MauricioMoraes/87d76577babd4e084cba70f63c04b07d

...they worked!


#3

Began setting up Postgres DB.

https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart

Checked v14 is actually officially supported by Superset (phew, it is)

https://superset.apache.org/docs/installation/configuring-superset

t $  sudo -i -u postgres 

postgres $  createdb test_db

# useradd test_db

Opting to share network with host to keep localhost connections simple, connecting via localhost:8088.

So to build and run:

$ cd ~/superset_experiment.git/

t $ docker build .

postgres $ psql

postgres=# CREATE USER test_superset WITH PASSWORD 'secret_password';

Then connect to it via GUI. The container is now connected to the DB!

t $ docker commit __________ apache/superset:dockerfile_psql

Then to run at startup:

t $ docker run -d  --network=host apache/superset:dockerfile_psql



#4

Onto sourcing some data.

Went for data from data.police.uk

Downloaded a data set of all police forces 2022, and checked it out manually.

Found the GMP data was incomplete - 2022-08 only, only 1 crime featured.

Went directly to gmp.police.uk website, but data was more about stats of police behavior.

Tried to find info about a specific stabbing on police's own website but did not appear.

Tried earlier data of 2020, but the .zip was empty

Moved on to Essex 2022-12 data, found it is more comprehensive.


#5

Onto importing data.

Found tutorial:

https://www.postgresqltutorial.com/postgresql-tutorial/import-csv-file-into-posgresql-table/

And examples in official docs:

https://www.postgresql.org/docs/current/sql-createtable.html

Put SQL commands into a file, generated a table that fits CSV with:

$ psql -d test_db -a -f /tmp/create_table_police_data.sql

Imported CSV to table with:

$ psql -d test_db -a -f /tmp/import_csv_police_data.sql

OK the data appears to have fit OK, when running:

t $ docker run -d  --network=host --name 405bd7434a1e

postgres=# SELECT longitude,latitude FROM crime_data_essex;

Hit some permission errors when trying to access via Supetset, found fix at:

https://www.postgresql.r2schools.com/error-permission-denied-for-table-table_name-in-postgresql/

GRANT ALL PRIVILEGES ON TABLE crime_data_essex TO test_superset;

Data is now accessible from within Apache Superset!


#6

Onto visualising data...

Would like to use co-ordinates to plot to map.

Fixing a mistake in column naming (used a generic Postgres value name	):

test_db=# ALTER TABLE crime_data_essex
test_db-# RENAME COLUMN type TO crime_type;

Created a dataset of violence crime via SQL statement:

SELECT longitude,latitude,crime_type FROM crime_data_essex WHERE crime_type LIKE 'Violence%';

There has been a mismatch between expected data types, whilst trying to plot to a map.
A weakness of Superset is it doesn't tell you what data type the chart is expecting?

Found a recommendation for data types at:

https://stackoverflow.com/questions/1196415/what-datatype-to-use-when-storing-latitude-and-longitude-data-in-sql-databases

Ah it turns out another error in data, as some fields are NULL.

So new SQL statement:

SELECT longitude,latitude,crime_type
FROM crime_data_essex
WHERE longitude is NOT NULL
AND latitude is NOT NULL
AND crime_type LIKE 'Violence%';

Discovered another bug.
If the Docker container shares with the host, this breaks the container's internet connectivity.

So back to:

t $ docker run -d -p 8080:8088 apache/superset:dockerfile

Realised it was not actually a bug, just a lack of Mapbox API key!

So now running Superset with:

t $ docker run -d -e MAPBOX_API_KEY="pk.eyJ1IjoieHphLXRlc3QiLCJhIjoiY2xkbjNlcmllMDRrczNxcGZ0Y29uMml1biJ9.t3sAoIppdcTXU0OJzZXbmA" -p 8080:8088 apache/superset:dockerfile


#7

Moved onto London Met data import.

Since the table template is already setup nicely, it is just a matter of:

$ cat 2022-*.csv > all_records.csv

However, because there are CSV headers that get duplicated:

$ head -1 all_records.csv > all_records_clean.csv

$ grep -v "^Crime ID,Month," all_records.csv >> all_records_clean.csv

Realised there is a more catch-all way to grant Postgres privs:

test_db=# GRANT pg_read_all_data TO test_superset;


#8

Moved back to GMP police data import.

Realised the police data is split across 2 files per month:

1. The initial log of a crime scene

2. The later outcome of the investigation

It seems that both have to be considered, to get accurate info.

They share a unique crime ID value.

Adding a DNS name to the Docker run now.

t $ docker run -d -e MAPBOX_API_KEY="pk.eyJ1IjoieHphLXRlc3QiLCJhIjoiY2xkbjNlcmllMDRrczNxcGZ0Y29uMml1biJ9.t3sAoIppdcTXU0OJzZXbmA" --add-host=database:192.168.1.24 -p 8080:8088 apache/superset:dockerfile

Realised the outcomes of the investigations can be categorised as follows:

----

Red-alert crimes:

Offender given suspended prison sentence
Offender sent to prison
Suspect charged


Gray-area crimes:
Offender given community sentence


Ambiguous crimes:

Court result unavailable
Court case unable to proceed
Action to be taken by another organisation
Investigation complete; no suspect identified


Not crimes(?):

Defendant found not guilty
Formal action is not in the public interest

----


9#

Imported all 2018-01 to 2019-06 GMP data, after joining the CSVs with Shell.

Building new SQL statement, based on joining the two data sources:

SELECT
crime_data_gmp_street.longitude,
crime_data_gmp_street.latitude,
crime_data_gmp_outcomes.outcome_type
FROM crime_data_gmp_street
JOIN crime_data_gmp_outcomes
ON
crime_data_gmp_street.crime_id =
crime_data_gmp_outcomes.crime_id
WHERE outcome_type
LIKE 'Offender given suspended prison sentence'
OR outcome_type
LIKE 'Offender sent to prison';

Plotted to map to view high crime areas.

Then imported the 2016-07 to 2018-12 GMP data, this time ran `sort *.csv | uniq` beforehand

Did a big SELECT COUNT(*) statement to ensure the numbers are reasonable (no duplicates).

Plotted 2016-07 to 2019-06 to map, using same SQL statement.
